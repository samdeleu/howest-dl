


get_ipython().run_line_magic("matplotlib", " inline")
import os

# Numbers
import pandas as pd
import numpy as np

# Graphics
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
from termcolor import (
    colored,
    cprint,
)

# Modeling
from sklearn.preprocessing import (
    MinMaxScaler,
    StandardScaler,
    OneHotEncoder,
    LabelBinarizer,
)
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
)

from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight


import tensorflow as tf

from keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Activation,
    BatchNormalization,
)
from keras.callbacks import EarlyStopping

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import image


###### Voor Tensorflow-GPU ########
import re

#K.set_image_dim_ordering('tf')

# Make sure that we have pandas.DataFrames at each step in scikit-learn
from sklearn import set_config
set_config(transform_output="pandas")

# Multiprocessing options (joblib/loky)
os.environ['OMP_NUM_THREADS']='8'

# limit some output when printing pandas data
pd.set_option('display.max_rows',200)
pd.set_option('display.max_columns',200)
pd.set_option('mode.copy_on_write', True)
pd.set_option('future.no_silent_downcasting', True)

# Some defaults for matplotlib
plt.rcParams['image.cmap'] = 'gray'
# plt.tight_layout()
# plt.rcParams.update({
#     "axes.titlesize":"10",
#     "xtick.labelsize":"8",
#     "ytick.labelsize":"8",
#     "font.size": "8",
#     # "figure.figsize": [6,3],
# })


# Helpers
def display_title(title, h=1):
    match h:
        case 1:
            cprint(title, "blue", "on_light_grey")
        case 2:
            cprint(title, "black", "on_cyan")
        case 3:
            cprint(title, "black", "on_light_yellow")
        case _:
            cprint(title, "white", "on_light_magenta")

def display_title2(title, h=1):
    match h:
        case 1:
            print(colored(title, "blue"))
        case 2:
            print(colored(title, "red"))
        case 3:
            print(colored(title, "green"))
        case _:
            print(colored(title, "yellow"))




display_title("een")
display("aaaaaaa")
display_title("twee", h=2)
display("bbbb bbbbb bbb")
display_title("drie", h=3)
display("ccc ccc ccccc cc c cccc")
display_title("vier", h=4)
display("ddd dd ddddddd dddddddd dddd")






# Import customer satisfaction datataset

dataset = pd.read_csv('bank.csv',delimiter=';')
print(f"Dataset shape {dataset.shape}")
display(dataset.tail())



# Statistical description of the dataset:
print(f"Dataset shape {dataset.shape}")
dataset.describe()


# in practise the duration column is not available. That's why it needs to be removed from the dataset

# remove the 'duration' column

dataset.drop('duration',axis=1, inplace=True)
print(f"Dataset shape {dataset.shape}")


pd.set_option('future.no_silent_downcasting', True)
# replace yes/no by 1/0 
dataset['y'] = dataset['y'].replace({'no':0,'yes':1})
dataset['loan'] = dataset['loan'].replace({'no':0,'yes':1})
dataset['default'] = dataset['default'].replace({'no':0,'yes':1})
dataset['housing'] = dataset['housing'].replace({'no':0,'yes':1})

print(f"Dataset shape {dataset.shape}")
display(dataset)


# Apply one-hot encoding to the remaining categorical features. 

# Example: 
# one-hot encoding of job
dataset = pd.concat([dataset,pd.get_dummies(dataset['job'], prefix='job')],axis=1)
dataset.drop(['job'],axis=1, inplace=True)

# one-hot encoding of marital
dataset = pd.concat([dataset,pd.get_dummies(dataset['marital'], prefix='marital')],axis=1)
dataset.drop(['marital'],axis=1, inplace=True)

# one-hot encoding of education
dataset = pd.concat([dataset,pd.get_dummies(dataset['education'], prefix='education')],axis=1)
dataset.drop(['education'],axis=1, inplace=True)

# one-hot encoding of contact
dataset = pd.concat([dataset,pd.get_dummies(dataset['contact'], prefix='contact')],axis=1)
dataset.drop(['contact'],axis=1, inplace=True)

# one-hot encoding of month
dataset = pd.concat([dataset,pd.get_dummies(dataset['month'], prefix='month')],axis=1)
dataset.drop(['month'],axis=1, inplace=True)

# one-hot encoding of poutcome
dataset = pd.concat([dataset,pd.get_dummies(dataset['poutcome'], prefix='poutcome')],axis=1)
dataset.drop(['poutcome'],axis=1, inplace=True)

# Overview
print(f"Dataset shape {dataset.shape}")
display(dataset)


# Split dataset into features and targets
dataset["y"] = pd.to_numeric(dataset['y'], downcast='integer', errors='coerce')
dataset.hist(column='y')
y = dataset.y.values
X = dataset.drop('y',axis=1)

print(f"Features shape {X.shape}, type: {type(X)}")
print(f"Targets shape {y.shape}, type: {type(y)}")



# Split into training set and test set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 10000, random_state =0)

# One-hot encoding of the targets
y_train_orig= y_train  # keep the original labels for testing purposes
print(y_train_orig[:5])

y_train = to_categorical(y_train)
print(y_train[:5])



# Normalization of the features: Try Min-Max, standardscaler or robust scaler

# Standardscaler
#scaler = preprocessing.StandardScaler().fit(X_train)
#X_train = scaler.transform(X_train)
#X_test = scaler.transform(X_test)

# MinMax scaler

scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)



# Summary of sizes
display_title("Relative verdeling")
display(f"Feature set size: {X.shape}")
display(f"Target set size: {y.shape}")






# Train a Neural network on the training data. Use 20 percent of the training data as validation data.

# Initialize the neural network

## parameters of the NN
model = Sequential()
dropoutrate = 0.2
batchsize = 128
inputdim = X_train.shape[1]  #30 input features
print(f"Number of input features: {inputdim}")
adam = tf.keras.optimizers.Adam() # Adam optimizer

model.add(Dense(40, input_dim=inputdim, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(40, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(10, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

display(model.summary())



# Activating the model with optimizer, loss function and metrics to monitor
model.compile(
    loss=tf.keras.losses.categorical_crossentropy,
    optimizer=adam,
    metrics=['accuracy']
)

# Training
history = model.fit(
    X_train,
    y_train,
    epochs=50,
    validation_split=0.2,
    batch_size=batchsize,verbose=1
)



# Plot of the training history

# Accuray 
plt.plot(history.history['accuracy'],'r')
plt.plot(history.history['val_accuracy'],'b')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# Loss 
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'b')

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# Testing with the test set

y_pred = model.predict_classes(X_test)
print('\n')
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)

print(cf)
print(accuracy_score(y_test, y_pred) * 100) 

# ROC
probs = model.predict_proba(X_test)
# calculate the fpr and tpr for all thresholds of the classification

preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

# plot
#import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()






# Histogram of the classcount
label_count = np.bincount(y)

print('count klasse 0: ',label_count[0])
print('count klasse 1: ',label_count[1])

print('ratio: ',label_count[0]/label_count[1])


sns.catplot(x='y',kind='count', data=dataset)


# Train a Neural network on the training data. Use 20 percent of the training data as validation data.

# Initialize the neural network

## parameters of the NN
model = Sequential()
dropoutrate = 0.2
batchsize = 64
epochs = 10
inputdim = X_train.shape[1]  #30 input features
adam = tf.keras.optimizers.Adam() # Adam optimizer


model.add(Dense(50, input_dim=inputdim, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(50, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(50, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])

class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train_orig),y_train_orig)
print(class_weights)
class_weights= dict(enumerate(class_weights))


history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, verbose=1,class_weight=class_weights)



# Plot history

# Accuray 
plt.plot(history.history['accuracy'],'r')
plt.plot(history.history['val_accuracy'],'b')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# Loss 
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'b')

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# Testing

y_pred = model.predict_classes(X_test)
print('\n')
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)

print(cf)
print(accuracy_score(y_test, y_pred) * 100) 

# ROC
probs = model.predict_proba(X_test)
# calculate the fpr and tpr for all thresholds of the classification

preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

# plot
#import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()






# Oversampling with SMOTE

from imblearn.over_sampling import BorderlineSMOTE
X_resampled, y_resampled = BorderlineSMOTE().fit_resample(X_train, y_train_orig)

y_resampled_orig = y_resampled
y_resampled = to_categorical(y_resampled)





# Train a Neural network on the training data. Use 20 percent of the training data as validation data.

# Initialize the neural network

## parameters of the NN
model = Sequential()
dropoutrate = 0.2
batchsize = 64
inputdim = X_train.shape[1]  #30 input features
adam = tf.keras.optimizers.Adam() # Adam optimizer
#sgd = tf.keras.optimizers.sgd(lr=0.001)


model.add(Dense(40, input_dim=inputdim, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(40, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(10, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])

# Training

history = model.fit(X_resampled, y_resampled, epochs=50,validation_split=0.2, batch_size=batchsize,verbose=1)


# Plot history

# Accuray 
plt.plot(history.history['accuracy'],'r')
plt.plot(history.history['val_accuracy'],'b')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# Loss 
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'b')

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# Testing

y_pred = model.predict_classes(X_test)
print('\n')
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)

print(cf)
print(accuracy_score(y_test, y_pred) * 100) 

# ROC
probs = model.predict_proba(X_test)
# calculate the fpr and tpr for all thresholds of the classification

preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

# plot
#import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()




