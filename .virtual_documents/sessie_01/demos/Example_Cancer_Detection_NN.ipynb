get_ipython().run_line_magic("matplotlib", " inline")
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import preprocessing
from sklearn.preprocessing import RobustScaler

# Import Tensorflow libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical


# Import dataset
dataset = pd.read_csv('cancer.csv')
dataset.head(20)


# Preprocessing 
dataset.drop('id',axis=1, inplace=True)

# Drop the unnamed column
dataset.drop(dataset.columns[31],axis=1,inplace=True)

# diagnosis labels M -> 1 and B -> 0
dataset.diagnosis.replace(['M', 'B'], [1, 0], inplace=True)


# Split into features and targets

y = dataset.diagnosis
X = dataset.drop('diagnosis',axis=1)



# Create training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state =0)

# convert to numpy array
X_train = X_train.values
X_test = X_test.values

# One-hot encoding of the targets
y_train = to_categorical(y_train)

# Normalize the features
# via robustscaler: This Scaler removes the median and scales the data according to the quantile range 

Rscaler = RobustScaler()
Rscaler.fit(X_train)

X_train = Rscaler.transform(X_train)
X_test = Rscaler. transform(X_test)



# Initialize the neural network

## parameters of the NN
model = Sequential()
dropoutrate = 0.2
batchsize = 16
inputdim = X_train.shape[1]  #30 input features
adam = tf.keras.optimizers.Adam()
sgd = tf.keras.optimizers.SGD()


model.add(Dense(10, input_dim=inputdim, kernel_initializer='uniform',activation='relu'))
#model.add(Dropout(dropoutrate))
#model.add(BatchNormalization())
model.add(Dense(40, kernel_initializer='uniform',activation='relu'))
#model.add(Dropout(dropoutrate))
#model.add(BatchNormalization())
model.add(Dense(40, kernel_initializer='uniform',activation='relu'))
model.add(Dropout(dropoutrate))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

model.compile(loss=tf.keras.losses.categorical_crossentropy,optimizer=adam,metrics=['accuracy'])

# Training

history = model.fit(X_train, y_train, epochs=80,validation_split=0.3, batch_size=batchsize,verbose=1)


# Testing

y_pred = model.predict_classes(X_test)
print('\n')
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)

print(cf)
print(accuracy_score(y_test, y_pred) * 100) 


# Plot history

# Accuray 
plt.plot(history.history['accuracy'],'r')
plt.plot(history.history['val_accuracy'],'b')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# Loss 
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'b')

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()






