get_ipython().run_line_magic("matplotlib", " inline")

import os

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn import linear_model, datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# distributions
from scipy.stats import randint 
from scipy.stats import uniform

#Bayessearch cv uit Scikit-optimize
from skopt import BayesSearchCV


from sklearn.svm import SVC
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
# Import Keras libraries

# Import Tensorflow libraries
import keras
from keras.models import Sequential
from keras.layers import Activation
from keras.optimizers import SGD
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import BatchNormalization
from keras.layers import MaxPooling2D
from keras import backend as K
from keras.utils import np_utils


# Import dataset
dataset = pd.read_csv('cancer.csv')
dataset.head()


# Data preprocessing
dataset.drop('id',axis=1, inplace=True)

# Remove right column (unnamed)
dataset.drop(dataset.columns[31],axis=1,inplace=True)

# diagnosis labels M -> 1 and B -> 0
dataset.diagnosis.replace(['M', 'B'], [1, 0], inplace=True)


# Split dataset up in features and targets
y = dataset.diagnosis
X = dataset.drop('diagnosis',axis=1)


# Create training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state =0)

# feature normalization
# via robustscaler: This Scaler removes the median and scales the data according to the quantile range 

Rscaler = RobustScaler()
Rscaler.fit(X_train)

X_train = Rscaler.transform(X_train)
X_test = Rscaler. transform(X_test)



# Train an SVM (grid search)

model = SVC()
parameters = [ 
        {'kernel': ['linear'], 'C': np.linspace(1,20,10)},
        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},
        {'kernel': ['poly'], 'C':[1, 10]} ]
grid_search = GridSearchCV(estimator = model, 
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 4,
                           n_jobs = -1,
                           verbose = 3)
grid_search = grid_search.fit(X_train, y_train)

best_accuracy = grid_search.best_score_ 
best_parameters = grid_search.best_params_  

print('Best accuracy : ', grid_search.best_score_)
print('Best parameters :', grid_search.best_params_  )

y_pred = grid_search.predict(X_test)
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)
print(cf)
print(accuracy_score(y_test, y_pred) * 100) 





# Train an SVM (random search)

model = SVC()
parameters = {'kernel': ['linear','rbf','poly'],
              'C': uniform(0.01, 100), 
              'gamma': uniform(0.001, 2)}


n_iter_search = 40


random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=4,n_iter=n_iter_search)

random_search = random_search.fit(X_train, y_train)

best_accuracy = random_search.best_score_ 
best_parameters = random_search.best_params_  

print('Best accuracy : ', random_search.best_score_)
print('Best parameters :',random_search.best_params_  )

y_pred = random_search.predict(X_test)
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)
print(cf)
print(accuracy_score(y_test, y_pred) * 100) 





# Train an SVM (Bayes optimization )

model = SVC()
parameters = {'kernel': ['linear','rbf','poly'],
              'C': (0.01, 100,'uniform'), # 
              'gamma': (0.001, 2,'uniform')}

n_iter_search = 20

Bayes_search = BayesSearchCV(model,parameters,n_iter=n_iter_search,cv=4,verbose=1)


Bayes_search.fit(X_train, y_train)

best_accuracy = Bayes_search.best_score_ 
best_parameters = Bayes_search.best_params_  

print('Best accuracy : ', Bayes_search.best_score_)
print('Best parameters :',Bayes_search.best_params_  )

y_pred = Bayes_search.predict(X_test)
print(classification_report(y_test, y_pred))

cf = confusion_matrix(y_test, y_pred)
print(cf)
print(accuracy_score(y_test, y_pred) * 100) 







# Import dataset
dataset = pd.read_csv('cancer.csv')
dataset.head()


# Data preprocessing
dataset.drop('id',axis=1, inplace=True)

# Remove right column (unnamed)
dataset.drop(dataset.columns[31],axis=1,inplace=True)

# diagnosis labels M -> 1 and B -> 0
dataset.diagnosis.replace(['M', 'B'], [1, 0], inplace=True)


# Split dataset up in features and targets
y = dataset.diagnosis
X = dataset.drop('diagnosis',axis=1)




# Create training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state =0)

# Convert to numpy array
X_train = X_train.values
X_test = X_test.values

# One-hot encoding of the targets
y_train = np_utils.to_categorical(y_train)

# Feature normalization
# via robustscaler: This Scaler removes the median and scales the data according to the quantile range 

Rscaler = RobustScaler()
Rscaler.fit(X_train)

X_train = Rscaler.transform(X_train)
X_test = Rscaler. transform(X_test)


def create_model(optimizer='adam',activation = 'relu',dropout_rate = 0.0, kernel_initializer='uniform',neurons = 10):
    model = Sequential()
    model.add(Dense(neurons, input_dim=X_train.shape[1], kernel_initializer=kernel_initializer,activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(BatchNormalization())
    model.add(Dense(neurons, kernel_initializer=kernel_initializer,activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(BatchNormalization())
    model.add(Dense(neurons, kernel_initializer=kernel_initializer,activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(BatchNormalization())
    model.add(Dense(2, activation='softmax'))

    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer,metrics=['accuracy']) 
    return model


from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV


model = KerasClassifier(build_fn=create_model, batch_size=8, epochs =10)

input_dim= [X_train.shape[1]]
activation =  ['relu', 'tanh', 'sigmoid'] 
#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
dropout_rate = [0.0, 0.1, 0.5]
#weight_constraint=[1, 2, 3, 4, 5]
neurons = [5, 10, 20, 30, 40 ,50]
init = ['uniform', 'lecun_uniform', 'normal']
optimizer = [ 'SGD', 'adam']
#optimizer = [ 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']

epochs = [50] 
batch_size = [8, 16, 32] 
param_grid = dict(epochs=epochs, batch_size=batch_size,optimizer = optimizer, dropout_rate = dropout_rate,activation = activation)
##############################################################
grid = GridSearchCV(estimator=model, param_grid = param_grid,verbose=3)
grid_result = grid.fit(X_train, y_train) 




print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))



