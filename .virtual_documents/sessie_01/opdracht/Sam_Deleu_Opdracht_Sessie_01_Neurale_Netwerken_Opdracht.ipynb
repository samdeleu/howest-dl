








get_ipython().run_line_magic("matplotlib", " inline")
from datetime import datetime
import pytz
import time

# Graphics
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from termcolor import (
    colored,
    cprint,
)
import matplotlib.image as mpimg
from skimage.io import imread, imshow

# Data
import numpy as np
import pandas as pd

# SKLearn
from sklearn.datasets import (
    make_blobs,
)
from sklearn.model_selection import (
    train_test_split,
    GridSearchCV,
    RandomizedSearchCV,
)
from sklearn.preprocessing import (
    MinMaxScaler,
    StandardScaler,
    OneHotEncoder,
    LabelBinarizer,
)
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    auc,
    roc_curve,
    RocCurveDisplay,
)

from sklearn.utils import (
    class_weight,
)

# distributions
from scipy.stats import randint 
from scipy.stats import uniform

# Tensorflow and Keras
import tensorflow as tf
from tensorflow.keras.models import (
    Sequential,
)

from tensorflow.keras.layers import (
    Activation,
    Input, Dense,
    Dropout,
    BatchNormalization,
    Conv2D, MaxPooling2D,
)

from tensorflow.keras.optimizers import (
    SGD,
    Adam,
)

from tensorflow.keras.activations import (
    leaky_relu,
)

from tensorflow.keras.callbacks import (
    EarlyStopping,
    ModelCheckpoint,
    ReduceLROnPlateau,
)

from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import image

# Some defaults for matplotlib
LARGE = 12
MEDIUM = 10
SMALL = 6
plt.tight_layout()
plt.rcParams.update({
    'axes.titlesize': MEDIUM,
    'legend.fontsize': SMALL,
    'figure.figsize': (5, 3),
    'axes.labelsize': MEDIUM,
    'axes.titlesize': MEDIUM,
    'xtick.labelsize': SMALL,
    'ytick.labelsize': SMALL,
    'figure.titlesize': LARGE
})

# Reproducibility of the results
SEED = 42
tf.config.experimental.enable_op_determinism()
tf.random.set_seed(SEED)

###### Voor Tensorflow-GPU ########
physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
print(physical_devices)





# Display Helpers
def display_title(title, value=None):
    if value is None:
        cprint(title, "black", "on_cyan")
    else:
        print(colored(title, "blue"))
        print(value)

def display_value(title, value):
    print(f"{colored(title, "blue")}: {value}")

display_title("Title")
display_title("Value", value=1)
display_value("Today is", datetime.now(pytz.timezone("Europe/Brussels")).strftime("%Y-%m-%d %H:%M:%S"))






# Plotting
def plot_history(history, title="Results"):
    """Plot error and accuracy of model.fit """
    fig, axes = plt.subplots(1, 2, figsize = (12, 3), sharey=False)

    # Loss (Training error)
    axes[0].set_title("Loss")
    axes[0].plot(history["loss"], "red", linewidth=2.0, label="training loss")
    axes[0].plot(history['val_loss'],'blue',linewidth=2.0, label="validation loss")
    axes[0].set_xlabel('epoch')
    axes[0].set_ylabel('error')
    axes[0].grid()
    axes[0].legend(loc='upper right')
    
    # Accuracy
    axes[1].set_title("Accuracy")
    axes[1].plot(history["accuracy"], "red", linewidth=2.0, label="training accuracy")
    axes[1].plot(history['val_accuracy'],'blue',linewidth=2.0, label="validation accuracy")
    axes[1].set_xlabel('epoch')
    axes[1].set_ylabel('accuracy')
    axes[1].grid()
    axes[1].legend(loc="lower right")

    # Overall title
    fig.suptitle(title, verticalalignment="top")

    # show
    plt.show()


# ROC curve
def plot_roc_curve(model, X_test, y_test, title="ROC Curve"):
    display_title(title)

    print("y_test")
    print(y_test.shape)
    print(y_test)
    # ROC
    y_pred_proba = model.predict(X_test)
    print("y_pred_proba")
    print(y_pred_proba.shape)
    print(y_pred_proba)
    y_pred_class = np.argmax(y_pred_proba, axis=1)
    print("y_pred_class")
    print(y_pred_class.shape)
    print(y_pred_class)
    y_pred_true = y_pred_proba[:, 1]  # De kans op 1
    print("y_pred_true")
    print(y_pred_true.shape)
    print(y_pred_true)
    
    # calculate the fpr and tpr for all thresholds of the classification
    fpr, tpr, threshold = roc_curve(y_test, y_pred_true)
    roc_auc = auc(fpr, tpr)
    
    # plot
    #import matplotlib.pyplot as plt
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()


# Plot decision boundary
def plot_decision_boundary(model, X_orig, y_orig, title="Results"):
    # Plotten van de decision boundary

    # build a grid
    h = 0.2
    x1_min = X_orig[:,0].min()-2
    x1_max = X_orig[:,0].max()+2
    x2_min = X_orig[:,1].min()-2
    x2_max = X_orig[:,1].max()+2
    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, h),np.arange(x2_min, x2_max, h))
    # big list of point coordinates for the grid
    xy = np.vstack([xx.ravel(), yy.ravel()]).T
    
    grid_predict_proba = model.predict(xy)
    grid_predict_class = np.argmax(grid_predict_proba, axis=1)
    # reshape to xx or yy
    reshaped_grid_predict_class = grid_predict_class.reshape(xx.shape)

    fig = plt.figure(figsize=(8, 4))
    # color plot of grid data
    plt.pcolormesh(xx, yy, reshaped_grid_predict_class, cmap='rainbow')
    
    # plot the original dataset again (make edges black)
    plt.scatter(X_orig[:, 0], X_orig[:, 1], c=y_orig, edgecolor='k', s=20, cmap='rainbow')
    
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.suptitle(title)
    plt.show()





# Metrics
def print_metrics(model, X_test, y_test, title="Results"):
    display_title(title)
    y_pred_proba = model.predict(X_test)
    y_pred_class = np.argmax(y_pred_proba, axis=1)

    display_title("classification report", classification_report(y_test, y_pred_class))
    display_title("confusion matrix", confusion_matrix(y_test, y_pred_class))
    display_value("Accuracy score", (accuracy_score(y_test, y_pred_class) * 100))





X, y = make_blobs(n_samples=1000, centers=4, center_box = (-10,10),
                  random_state=0, cluster_std=1)
plt.scatter(X[:, 0], X[:, 1], s=10, c=y, cmap="rainbow");


# splitsen in test set en training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =0)

# ---------------------------------------------------
# normaliseren van de input data

#scaler = StandardScaler()

#scaler.fit(X_train)
#X_train = scaler.transform(X_train)
#X_test = scaler.transform(X_test)

#----------------------------------------------------
# one-hot encoding op output
y_train = to_categorical(y_train)
display(y_train)





# NN parameters

input_dim = X_train.shape[1]  # input layer size
batch_size = 32               # nbr of samples presented before adopting the weights
epochs = 100                  # nbr of times all data is presented

dropoutrate = 0.0             # neuron dropout on the different layers

activation_hidden = 'relu'    # activatiefunctie van de hidden layer neuronen
activation_output = 'softmax' # activatie van de output layer neuronen
initializer = 'RandomUniform' # type van kernel intializer

SGD = tf.keras.optimizers.SGD(learning_rate=0.3)  # gradient descend with momentum optimizer

# Model
model = Sequential()

# Input Layer
model.add(Input(shape=(input_dim,)))
          
# Layer 1
model.add(Dense(20, kernel_initializer=initializer,activation=activation_hidden))
model.add(Dropout(dropoutrate))

# layer 2
model.add(Dense(20, kernel_initializer=initializer,activation=activation_hidden))
model.add(Dropout(dropoutrate))

# Layer 3
model.add(Dense(20, kernel_initializer=initializer,activation=activation_hidden))
model.add(Dropout(dropoutrate))

# Layer 4
model.add(Dense(20, kernel_initializer=initializer,activation=activation_hidden))
model.add(Dropout(dropoutrate))

# Output Layer
model.add(Dense(y_train.shape[1], kernel_initializer=initializer,activation=activation_output))

display_title("Model 1")
display(model.summary())




# Build the model
model.compile(
    loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(),
    metrics=['accuracy'],
)



# train the model
history = model.fit(
    X_train, y_train,
    epochs=epochs,
    validation_split = 0.2,  # use 20% of the training data used for validation
    verbose=1
)


# Display Results
# History of training process
plot_history(history.history, title="Model 1")
# Metrics with test set
print_metrics(model, X_test, y_test, title="Model 1")

# Graphical represenation of resulting split
plot_decision_boundary(model, X, y, title="Model 1")











def nn_architecture_1(
    # NN layout parameters
    name,                              # descriptive name of the layout
    input_dimension,                   # input layer size
    output_dimension,                  # output layer size
    number_of_hidden_layers,           # Number of hidden "identical" layers
    hidden_layer_neurons,              # nbr of neurons in a hidden layer
    dropoutrate,                       # neuron dropout on the different layers
    activation_function_hidden_layer,  # activatiefunctie van de hidden layer neuronen
    activation_function_output_layer,  # activatie van de output layer neuronen
    kernel_initializer,                # type van kernel intializer
    optimizer,                         # optimizer to adjust weights
    batch_normalization = False,       # add batch normalization to each row
):
    """ Sequential Architecture with hidden layers of same size
        the loss function is fixed on categorical_crossentropy
        the metric used for evaluation is 'accuracy'
    """
    # Model
    model = Sequential()
    
    # Input Layer
    model.add(Input(shape=(input_dimension,)))
    if batch_normalization:
        model.add(BatchNormalization())

    # Hidden layers
    for _ in range(number_of_hidden_layers):
        model.add(Dense(hidden_layer_neurons,
                        kernel_initializer=kernel_initializer,
                        bias_initializer="zeros",
                        activation=activation_function_hidden_layer))
        if batch_normalization:
            model.add(BatchNormalization())
        model.add(Dropout(dropoutrate))
    
    # Output Layer
    model.add(Dense(output_dimension,
                    kernel_initializer=kernel_initializer,
                    activation=activation_function_output_layer))
    
    # display_title(name)
    # display(model.summary())

    # Build the model
    model.compile(
        loss='categorical_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy'],
    )

    return model






def deel_1_variatie_learning_rate(
    experiment = "EXP",
    learning_rate = 0.0001,
    momentum = 0.0,
    nesterov = False,
    batch_normalization = False,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment} - A1 SGD LR({learning_rate}) Momentum({momentum}) Nesterov({nesterov}) Norm({batch_normalization})"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = 20,
        dropoutrate = 0.0,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov),
        batch_normalization = batch_normalization,
    )
    
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=0
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name)    





deel_1_variatie_learning_rate(experiment = "1-1", learning_rate = 0.001, momentum = 0.0, nesterov = False, batch_normalization = False)


deel_1_variatie_learning_rate(experiment = "1-2", learning_rate = 0.01, momentum = 0.0, nesterov = False, batch_normalization = False)


deel_1_variatie_learning_rate(experiment = "1-3", learning_rate = 0.1, momentum = 0.0, nesterov = False, batch_normalization = False)


deel_1_variatie_learning_rate(experiment = "1-4", learning_rate = 0.9, momentum = 0.0, nesterov = False, batch_normalization = False)


deel_1_variatie_learning_rate(experiment = "1-5", learning_rate = 0.001, momentum = 0.5, nesterov = True, batch_normalization = True)


deel_1_variatie_learning_rate(experiment = "1-6", learning_rate = 0.1, momentum = 0.5, nesterov = True, batch_normalization = True)








def deel_2_variatie_optimizers(
    experiment,
    optimizer,
    callbacks = None,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment}"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = 20,
        dropoutrate = 0.0,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=optimizer,
        batch_normalization = True,
    )
    
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 


deel_2_variatie_optimizers(experiment = "2-1: Adam LR=0.001", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))


deel_2_variatie_optimizers(experiment = "2-2: Adam LR=0.01", optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))


deel_2_variatie_optimizers(experiment = "2-3: Adam LR=0.1", optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))


# Gebruik van callbacks.
# Effect van early stopping op de training time
callbacks = [
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=5,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
]
    
deel_2_variatie_optimizers(
    experiment = "2-4: Adam LR=0.001 with EarlyStopping",
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    callbacks=callbacks,
)


callbacks = [
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=5,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
]
    
deel_2_variatie_optimizers(
    experiment = "2-5: RMSProp LR=0.001 with EarlyStopping",
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),
    callbacks=callbacks,
)


callbacks = [
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=5,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
]
    
deel_2_variatie_optimizers(
    experiment = "2-5: Adagrad LR=0.001 with EarlyStopping",
    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),
    callbacks=callbacks,
)








def deel_3_dropout_rate(
    experiment,
    optimizer,
    dropoutrate,
    callbacks = None,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment}"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = 20,
        dropoutrate = dropoutrate,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=optimizer,
        batch_normalization = True,
    )
    
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 


callbacks = [
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=5,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
]
    
deel_3_dropout_rate(
    experiment = "3-1: Adagrad LR=0.001, EarlyStopping, dropoutrate=0.8",
    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001),
    dropoutrate = 0.8,
    callbacks = callbacks,
)


callbacks = [
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=5,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
]

deel_3_dropout_rate(
    experiment = "3-2: Adam LR=0.001 with EarlyStopping, dropoutrate=0.8",
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    dropoutrate = 0.8,
    callbacks = callbacks,
)








def deel_4_aantal_neuronen(
    experiment,
    neurons_in_hidden_layer,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment} Neurons({neurons_in_hidden_layer}) A1 Layers(4) Adam LR(0.001) Norm(True)"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = neurons_in_hidden_layer,
        dropoutrate = 0.0,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        batch_normalization = True,
    )
    nn_model.summary()
    callbacks = [
        EarlyStopping(
            monitor="val_loss",   # metric to monitor
            patience=5,           # stop when no improvement after 5 consecutive epochs
            mode="min",           # stop when metric stops decreasing
            restore_best_weights=True,
            verbose=1,            # display the actions taken
        ),
    ]
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 


deel_4_aantal_neuronen(experiment="4-1", neurons_in_hidden_layer=2)


deel_4_aantal_neuronen(experiment="4-2", neurons_in_hidden_layer=4)


deel_4_aantal_neuronen(experiment="4-3", neurons_in_hidden_layer=8)


deel_4_aantal_neuronen(experiment="4-4", neurons_in_hidden_layer=10)


deel_4_aantal_neuronen(experiment="4-5", neurons_in_hidden_layer=12)


deel_4_aantal_neuronen(experiment="4-6", neurons_in_hidden_layer=14)


deel_4_aantal_neuronen(experiment="4-7", neurons_in_hidden_layer=16)


deel_4_aantal_neuronen(experiment="4-8", neurons_in_hidden_layer=144)








def deel_5_aantal_hidden_layers(
    experiment,
    number_of_hidden_layers,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment} Layers({number_of_hidden_layers}) A1 Neurons(16) Adam LR(0.001) Norm(True)"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = number_of_hidden_layers,
        hidden_layer_neurons = 16,
        dropoutrate = 0.0,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        batch_normalization = True,
    )
    nn_model.summary()
    callbacks = [
        EarlyStopping(
            monitor="val_loss",   # metric to monitor
            patience=5,           # stop when no improvement after 5 consecutive epochs
            mode="min",           # stop when metric stops decreasing
            restore_best_weights=True,
            verbose=1,            # display the actions taken
        ),
    ]
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 


deel_5_aantal_hidden_layers(experiment="5-1", number_of_hidden_layers=1)


deel_5_aantal_hidden_layers(experiment="5-2", number_of_hidden_layers=2)


deel_5_aantal_hidden_layers(experiment="5-3", number_of_hidden_layers=4)


deel_5_aantal_hidden_layers(experiment="5-4", number_of_hidden_layers=8)


deel_5_aantal_hidden_layers(experiment="5-5", number_of_hidden_layers=16)


deel_5_aantal_hidden_layers(experiment="5-6", number_of_hidden_layers=32)











def deel_7_activatie_functie_hidden_layer(
    experiment,
    activation_function_hidden_layer,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment} - A1 Layers(4) Neurons(16) Adam LR(0.001) Norm(True)"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = 16,
        dropoutrate = 0.0,
        activation_function_hidden_layer = activation_function_hidden_layer,
        activation_function_output_layer = "sigmoid",
        kernel_initializer = "RandomUniform",
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        batch_normalization = True,
    )
    callbacks = [
        EarlyStopping(
            monitor="val_loss",   # metric to monitor
            patience=5,           # stop when no improvement after 5 consecutive epochs
            mode="min",           # stop when metric stops decreasing
            restore_best_weights=True,
            verbose=1,            # display the actions taken
        ),
    ]
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 


deel_7_activatie_functie_hidden_layer(
    experiment="7-1 ReLU(negative_slope=0.0)",
    activation_function_hidden_layer=tf.keras.layers.ReLU(negative_slope=0.0),
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-2 ReLU(negative_slope=0.5)",
    activation_function_hidden_layer=tf.keras.layers.ReLU(negative_slope=0.5),
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-3 LeakyReLU(negative_slope=0.5)",
    activation_function_hidden_layer=tf.keras.layers.LeakyReLU(negative_slope=0.5),
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-3 LeakyReLU(negative_slope=0.5)",
    activation_function_hidden_layer=tf.keras.layers.LeakyReLU(negative_slope=1.5),
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-4 ELU",
    activation_function_hidden_layer="elu",
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-5 Exponential",
    activation_function_hidden_layer="exponential",
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-5 Gauss Linear",
    activation_function_hidden_layer="gelu",
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-6 Linear(passthrough)",
    activation_function_hidden_layer="linear",
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-7 Scaled Exponential Linear Unit",
    activation_function_hidden_layer="selu",
)


deel_7_activatie_functie_hidden_layer(
    experiment="7-8 Silu",
    activation_function_hidden_layer="silu",
)


# Activatie met parameter(s)
deel_7_activatie_functie_hidden_layer(
    experiment="7-8 Leaky_ReLU met parameter",
    activation_function_hidden_layer=lambda x: leaky_relu(x, negative_slope=0.5)
)








def deel_8_initializers(
    experiment,
    kernel_initializer,
):
    """ De functie maakt gebruik van de globale X, y, X_test, y_test waarden """
    experiment_name = f"{experiment} - A1 Layers(4) Neurons(16) Adam LR(0.001) Norm(True)"
    nn_model = nn_architecture_1(
        # NN layout parameters
        name = experiment_name,
        input_dimension = X_train.shape[1],
        output_dimension = y_train.shape[1],
        number_of_hidden_layers = 4,
        hidden_layer_neurons = 16,
        dropoutrate = 0.0,
        activation_function_hidden_layer = "relu",
        activation_function_output_layer = "sigmoid",
        kernel_initializer = kernel_initializer,
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        batch_normalization = True,
    )
    callbacks = [
        EarlyStopping(
            monitor="val_loss",   # metric to monitor
            patience=5,           # stop when no improvement after 5 consecutive epochs
            mode="min",           # stop when metric stops decreasing
            restore_best_weights=True,
            verbose=1,            # display the actions taken
        ),
    ]
    # train the model
    history = nn_model.fit(
        X_train, y_train,
        epochs=100,
        callbacks=callbacks,
        validation_split = 0.2,  # use 20% of the training data used for validation
        verbose=1
    )
    
    # Display Results
    plot_history(history.history, title=experiment_name)
    print_metrics(nn_model, X_test, y_test, title=experiment_name)
    # plot_decision_boundary(nn_model, X, y, title=experiment_name) 



deel_8_initializers(
    experiment="8-1 RandomUniform",
    kernel_initializer="RandomUniform",
)


deel_8_initializers(
    experiment="8-2 RandomNormal (mean=0.0, stddev=0.05)",
    kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
)



deel_8_initializers(
    experiment="8-3 Zero",
    kernel_initializer=tf.keras.initializers.Zeros(),
)


deel_8_initializers(
    experiment="8-3 One",
    kernel_initializer=tf.keras.initializers.Ones(),
)


deel_8_initializers(
    experiment="8-4 Constant(3)",
    kernel_initializer=tf.keras.initializers.Constant(value=3.0),
)


deel_8_initializers(
    experiment="8-5 GlorotUniform",
    kernel_initializer=tf.keras.initializers.GlorotUniform(),
)


deel_8_initializers(
    experiment="8-5 HeNormal ",
    kernel_initializer=tf.keras.initializers.HeNormal(),
)











# Uitwerking opdracht customer satisfaction





# Read data
customer_dataset = pd.read_csv('./customersatisfaction.csv')

display_value("Observations/Features", customer_dataset.shape)
display(customer_dataset.describe())
display(customer_dataset.dtypes)
display(customer_dataset)


# Drop unusable features
display_value("customer_dataset['ID'].is_unique", customer_dataset["ID"].is_unique)
customer_dataset = customer_dataset.drop('ID', axis=1)
display_value("Observations/Features", customer_dataset.shape)


# De data is imbalanced
display_title("Data imbalanced")
customer_dataset.hist(column='TARGET')
counts = customer_dataset["TARGET"].value_counts()
counts_rel = customer_dataset["TARGET"].value_counts(normalize = True)
display_value("Value 0", f"{counts[0]} {counts_rel[0]:.2%}")
display_value("Value 1", f"{counts[1]} {counts_rel[1]:.2%}")

imbalance_ratio = counts[0]/counts[1]
display_value("Ratio 0/1", f"{imbalance_ratio:.3f}")



# Split into features and target
y = customer_dataset["TARGET"].to_numpy()
X = customer_dataset.drop("TARGET", axis=1)
display_value("Target(y)", y.shape)
display_value("Features(X)", X.shape)



# splitsen in test set en training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 10000, random_state = 0)
display_value("X_train", X_train.shape)
display_value("X_test", X_test.shape)
display_value("y_train", y_train.shape)
display_value("y_test", y_test.shape)



# normaliseren van de input data
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
display_title("X_train", X_train[:5])
display_title("X_test", X_test[:5])



# One-hot encoding of the target
y_train_orig = y_train    # keep the original labels for testing purposes and calculating the weights
y_train = to_categorical(y_train)
display_title("y_train original", f"{type(y_train_orig)}\n{y_train_orig[:5]}")
display_title("y_train categorical", f"{type(y_train)}\n{y_train[:5]}")





# NN Parameters
dropoutrate = 0.2
batch_size = 1024
epochs = 100
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
kernel_initializer = tf.keras.initializers.RandomUniform(minval = -0.05, maxval = 0.05)
loss_function = tf.keras.losses.categorical_crossentropy
# tf.keras.layers.ReLU(negative_slope=0.0)

input_dimension = X_train.shape[1]   # number of features in X_train
output_dimension = y_train.shape[1]  # number of categories in y_train


print(type(X_train), X_train.shape[1])
print(type(y_train), y_train.shape[1])
print(int(input_dimension))
print(int(input_dimension/2))
print(int(input_dimension/4))


print(np.unique(y_train_orig))
print(y_train_orig)      
# class_weights = class_weight.compute_class_weight(
#     "balanced",
#     np.unique(y_train_orig), # numpy array with the different classes
#     y_train_orig,  # orignal class labels for the samples
# )





# Model
customer_satisfaction_model = Sequential()
    
# Input Layer (dimension from number of features in X_train)
customer_satisfaction_model.add(Input(shape=(input_dimension,)))
customer_satisfaction_model.add(BatchNormalization())

# Hidden layers
# Layer 1
customer_satisfaction_model.add(
    Dense(
        units=int(input_dimension),
        kernel_initializer=kernel_initializer,
        bias_initializer="zeros",
        activation=tf.keras.layers.ReLU(negative_slope=0.0),
    )
)
customer_satisfaction_model.add(BatchNormalization())
customer_satisfaction_model.add(Dropout(dropoutrate))

# Layer 2
customer_satisfaction_model.add(
    Dense(
        units=int(input_dimension/2),
        kernel_initializer=kernel_initializer,
        bias_initializer="zeros",
        activation=tf.keras.layers.ReLU(negative_slope=0.0),
    )
)
customer_satisfaction_model.add(BatchNormalization())
customer_satisfaction_model.add(Dropout(dropoutrate))

# Layer 3
customer_satisfaction_model.add(
    Dense(
        units=int(input_dimension/2),
        kernel_initializer=kernel_initializer,
        bias_initializer="zeros",
        activation=tf.keras.layers.ReLU(negative_slope=0.0),
    )
)
customer_satisfaction_model.add(BatchNormalization())
customer_satisfaction_model.add(Dropout(dropoutrate))

# Layer 4
customer_satisfaction_model.add(
    Dense(
        units=int(input_dimension/2),
        kernel_initializer=kernel_initializer,
        bias_initializer="zeros",
        activation=tf.keras.layers.ReLU(negative_slope=0.0),
    )
)
customer_satisfaction_model.add(BatchNormalization())
customer_satisfaction_model.add(Dropout(dropoutrate))

# Output Layer (dimension from number of categories in y_train)
customer_satisfaction_model.add(
    Dense(
        units=output_dimension,
        kernel_initializer=kernel_initializer,
        activation="sigmoid"
    )
)
    
display_title("Customer Satisfaction Network")
display(customer_satisfaction_model.summary())

# Build the model
customer_satisfaction_model.compile(
    loss=loss_function,
    optimizer=optimizer,
    metrics=['accuracy'],
)






callbacks = [
    # Stop training when no further improvement is seen in the metric
    EarlyStopping(
        monitor="val_loss",   # metric to monitor
        patience=10,           # stop when no improvement after 5 consecutive epochs
        mode="min",           # stop when metric stops decreasing
        restore_best_weights=True,
        verbose=1,            # display the actions taken
    ),
    # Callback to save the Keras model or model weights at some frequency
    ModelCheckpoint(
        filepath="customer_satisfaction.keras",
        monitor="val_loss",
        mode="min",
        save_best_only=True,
        save_freq="epoch",
        verbose=1,
    ),
    # Reduce learning rate when a metric has stopped improving.
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        mode="min",
        # min_delta=0.0001,
        # cooldown=0,
        min_lr=1e-6,
        verbose=1,
    ),
]


# Take into account the imbalanced aspect of the data
# use sklearn.utils.class_weight
class_weights = class_weight.compute_class_weight(
    class_weight= "balanced",
    classes=np.unique(y_train_orig), # numpy array with the different classes
    y=y_train_orig,  # orignal class labels for the samples
)

class_weights= dict(enumerate(class_weights))
display_value("Class weights", class_weights)

# train the model
start_timing = time.time()
history = customer_satisfaction_model.fit(
    X_train, y_train,
    epochs=1000,
    callbacks=callbacks,
    validation_split = 0.2,  # use 20% of the training data used for validation
    batch_size=batch_size,
    class_weight=class_weights,
    verbose=1,
)
end_timing = time.time()

display_title("Total Training time", f"{end_timing - start_timing:.2f} seconds")
# print(customer_satisfaction_model.get_config())






# Display Results
plot_history(history.history, title="Customer Satisfaction Training")
print_metrics(customer_satisfaction_model, X_train, y_train_orig, title="Metrics on TRAINING SET")
print_metrics(customer_satisfaction_model, X_test, y_test, title="Metrics on TEST SET")
plot_roc_curve(customer_satisfaction_model, X_test, y_test, title="ROC Curve")
# plot_decision_boundary(nn_model, X, y, title=experiment_name) 








df_train = pd.read_csv('./fashion-mnist_train.csv')
df_test = pd.read_csv('./fashion-mnist_test.csv')
df_train.head()


# Uitwerking Zalando oefening








