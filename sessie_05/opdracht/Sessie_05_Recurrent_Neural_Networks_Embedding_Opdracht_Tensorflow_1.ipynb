{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opdracht Recurrent Neural Networks & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization,SpatialDropout1D,Bidirectional, Embedding, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "import re #regular expressions\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "\n",
    "# Voor GPU support\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Opsporen van storingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opdracht bestaat erin een LSTM te trainen voor het voorspellen van storingen in motoren. Het bestand 'storingen.csv' bevat de data.\n",
    "De dataset is tot stand gekomen door 100 motoren van hetzelfde type te bemeten en te tellen hoeveel cycli ze nog operationeel zijn vooraleer een storing zich manifesteert. \n",
    "\n",
    "\n",
    "De dataset is als volgt opgebouwd:\n",
    "- engine_ID = Het ID een bepaalde motor.\n",
    "- cycle = nummer van de cyclus van de motor.\n",
    "- setting1, setting2, setting3 = de verschillende operationele modi.\n",
    "- sensor1 -> sensor 21: metingen van de 21 verschillende sensoren.\n",
    "- ttf = time to failure: hoeveel cycli de motor nog kan draaien vooraleer er een storing optreedt.\n",
    "\n",
    "\n",
    "Een eerste benadering is het probleem te vereenvoudigen door het te vertalen naar een **classificatieprobleem**. Hierbij probeer je te voorspellen of de motor bijvoorbeeld binnen de komende 50 cycli al dan niet zal falen. \n",
    "\n",
    "\n",
    "De tweede benadering is het probleem beschouwen als een **regressieprobleem** waarbij je de resterende cycli (ttf) probeert te voorspellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Classificatie van de ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engine_id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "      <th>sensor17</th>\n",
       "      <th>sensor18</th>\n",
       "      <th>sensor19</th>\n",
       "      <th>sensor20</th>\n",
       "      <th>sensor21</th>\n",
       "      <th>ttf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.13</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   engine_id  cycle  setting1  setting2  setting3  sensor1  sensor2  sensor3  \\\n",
       "0          1      1   -0.0007   -0.0004     100.0   518.67   641.82  1589.70   \n",
       "1          1      2    0.0019   -0.0003     100.0   518.67   642.15  1591.82   \n",
       "2          1      3   -0.0043    0.0003     100.0   518.67   642.35  1587.99   \n",
       "3          1      4    0.0007    0.0000     100.0   518.67   642.35  1582.79   \n",
       "4          1      5   -0.0019   -0.0002     100.0   518.67   642.37  1582.85   \n",
       "\n",
       "   sensor4  sensor5  sensor6  sensor7  sensor8  sensor9  sensor10  sensor11  \\\n",
       "0  1400.60    14.62    21.61   554.36  2388.06  9046.19       1.3     47.47   \n",
       "1  1403.14    14.62    21.61   553.75  2388.04  9044.07       1.3     47.49   \n",
       "2  1404.20    14.62    21.61   554.26  2388.08  9052.94       1.3     47.27   \n",
       "3  1401.87    14.62    21.61   554.45  2388.11  9049.48       1.3     47.13   \n",
       "4  1406.22    14.62    21.61   554.00  2388.06  9055.15       1.3     47.28   \n",
       "\n",
       "   sensor12  sensor13  sensor14  sensor15  sensor16  sensor17  sensor18  \\\n",
       "0    521.66   2388.02   8138.62    8.4195      0.03       392      2388   \n",
       "1    522.28   2388.07   8131.49    8.4318      0.03       392      2388   \n",
       "2    522.42   2388.03   8133.23    8.4178      0.03       390      2388   \n",
       "3    522.86   2388.08   8133.83    8.3682      0.03       392      2388   \n",
       "4    522.19   2388.04   8133.80    8.4294      0.03       393      2388   \n",
       "\n",
       "   sensor19  sensor20  sensor21  ttf  \n",
       "0     100.0     39.06   23.4190  191  \n",
       "1     100.0     39.00   23.4236  190  \n",
       "2     100.0     38.95   23.3442  189  \n",
       "3     100.0     38.88   23.3739  188  \n",
       "4     100.0     38.90   23.4044  187  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inladen van de training set\n",
    "\n",
    "dataset = pd.read_csv('storingen.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "Voeg een extra 'failure' kolom toe aan de dataset die weergeeft of de motor al dan niet binnen de 50 cycli zal falen. Met andere woorden: is de ttf groter dan 50 is de waarde 0, is de ttf kleiner of gelijk aan 50, dan is de waarde gelijk aan 1.\n",
    "\n",
    "Bekijk ook in welke mate de data gebalanceerd is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toevoegen van 'failure' kolom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gebalanceerdheid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictie op basis van enkelvoudige data samples (geheugenloos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vooraleer de data als tijdreeks te beschouwen en LSTM of GRU modellen te trainen is het interessant om eerst te bekijken hoe goed je kan voorspellen met modellen die de temporele component niet gebruiken en elke meting onafhankelijk van de vorige beschouwen.\n",
    "Concreet betekent dit dat je voor een bepaald tijdstip op basis van de features de target probeert te voorspellen, zonder te kijken naar de voorgaande metingen. Elke rij in de dataset wordt onafhankelijk van de andere bekeken.\n",
    "\n",
    "- Splits de dataset op in een training set en een test set. De eerste 75 motoren worden gebruikt om te trainen, de andere motoren om te testen.\n",
    "- Train een logistic regression classifier en een random forest tree op de training set en evalueer met de test set. Bespreek de bekomen resultaten.\n",
    "- Train een vanilla neuraal netwerk, waarmee een klassiek feedforward neuraal netwerk wordt bedoeld. Evalueer en bespreek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opsplitsen in training set en test set\n",
    "\n",
    "\n",
    "\n",
    "# Normalisatie van de training set en test set\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Trees\n",
    "\n",
    "\n",
    "\n",
    "# Vanilla neural network\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voorspelling op basis van time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan vermoeden dat het al dan niet falen van de motoren binnen een zeker tijdinterval niet alleen bepaald wordt door de huidige meetwaarden maar voor een deel ook afgeleid kan worden uit het tijdsverloop van de meetwaarden.(=tijdreeks). Dit betekent dat de modellen rekening moeten houden met de huidige sample maar ook met een aantal voorgaande samples. Daarom zullen we onze toevlucht nemen tot recurrent neurals networks (LSTM of GRU) die via feedback verbindingen uitermate geschikt zijn om tijdsafhankelijkheden in time series te capteren.\n",
    "\n",
    "\n",
    "Doorloop de volgende stappen:\n",
    "\n",
    "- Opbouwen van de training set en test set:\n",
    "\n",
    "De training set en test set zullen hier bestaan uit sequenties met zekere lengte (seq_length). Neem om te starten een seq_length=30. Om deze sequenties op te bouwen ga je als volgt te werk:\n",
    "Overloop per engine_id de tijdreeks met een sliding window. Het window start bij de rij met de hoogste ttf en schuift rij per rij verder tot de motor faalt (laatste meting van die motor). De features die telkens binnen het window vallen vormen een sequentie (van in dit geval 30 waarden). Dit zijn de meest recente meetwaarde samen met 29 voorgaande meetwaarden. Bij de start zijn er nog geen voorgaande waarden, vandaar dat zero padding toegepast kan worden om toch tot een sequentie van 30 te komen. De bij een sequentie horende target waarde is de 'failure' waarde die hoort bij de meest recente meetwaarden in de sequentie.\n",
    "\n",
    "- Compileer een training set en een test set. \n",
    "\n",
    "De samples (=sequenties) afkomstig van de eerste 75 motoren komen in de training set, de resterende in de test set. \n",
    "\n",
    "\n",
    "- Normaliseer de training set en test set (bijvoorbeeld via een MinMAX scaler)\n",
    "\n",
    "\n",
    "- Trainen van een LSTM netwerk\n",
    "\n",
    "Train een LSTM op de training set en varieer de hyperparameters om een zo hoog mogelijke accuraatheid te bekomen. Mogelijke hyperparameters van het netwerk zijn de batch size, aantal layers, het aantal units per layer, dropout, recurrent dropout, optimizer, ...\n",
    "Probeer eveneens een bidirectional LSTM. Bijvoorbeeld: model.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "- Evalueer het LSTM netwerk en vergelijk de accuracy met deze van de niet-temporele modellen.\n",
    "\n",
    "- Optimaliseer het neuraal netwerk om het aantal false negatives te reduceren (een falen binnen de 50 cylussen dat niet ontdekt wordt)\n",
    "\n",
    "- Onderzoek of je met langere of kortere sequenties betere resultaten behaalt. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitwerking LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Regressie van de ttf\n",
    "\n",
    "- In plaats van te voorspellen of de motor zal falen binnen 50 cycli, voorspel je nu het aantal resterende cycli. De target is nu de time to failure (ttf).\n",
    "- Splits ook hier de dataset op in een training set en een test set. De data van de eerste 75 motoren wordt gebruikt om mee te trainen, de andere motoren om mee te testen.\n",
    "- Train een lineair regressiemodel en een random forest regressor op de training set en evalueer met de test set. Bespreek de bekomen resultaten in termen van Mean Absolute Error (MAE) en de R²-score.\n",
    "- Train vervolgens een LSTM. Dit is vergelijkbaar met wat je hebt gedaan bij classificatie. Let wel op dat je de juiste activatie- en loss functies gebruikt. Vergelijk de bekomen MAE en de R²-score met deze van de niet-temporele modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitwerking regressie van de ttf\n",
    "\n",
    "\n",
    "# Normalisatie van de training set en test set\n",
    "\n",
    "\n",
    "\n",
    "# Lineaire regressie\n",
    "\n",
    "\n",
    "# Random forest regressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictie van de resterende cycli via LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak op basis van de test set een histogram van de verschillen tussen de voorspelde ttf en de werkelijke ttf.\n",
    "Wat kan je daaruit besluiten? Lijkt het model eerder een onderschatting of een overschatting te maken van de ttf?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram van de error (y_pred - y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitbreiding: time series prediction met convolutional neural networks\n",
    "\n",
    "Als uitbreiding kan je proberen om een convolutional neural network te trainen op de sequenties. Elke sequenties kan je zien als een 2D array, gelijkaardig aan een afbeelding. \n",
    "Het trainen zal dus gelijkaardig zijn aan wat je gedaan hebt bij classificatie van images, alleen is de afbeelding hier een sequentie van sensorwaarden en de klasse het al dan niet falen binnen 50 cycli.\n",
    "\n",
    "Probeer ook het regressieprobleem uit via een convolutional neural network waarbij je de ttf voorspelt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificatie via een CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressie via CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text embedding met LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 VOORBEELD: Auteursidentificatie\n",
    "\n",
    "Het doel is om een LSTM/GRU netwerk te trainen zodat het in staat is de schrijver van een stukje tekst de identificeren\n",
    "De data is te vinden in 'author_dataset.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inlezen van de dataset\n",
    "\n",
    "dataset = pd.read_csv('./author_dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Text tokenization:\n",
    "- Maak een Keras tokenizer aan\n",
    "- Train de tokenizer op de teksten: tokenizer.fit_on_texts(X), met X de teksten\n",
    "- Converteer de teksten naar sequences: tokenizer.texts_to_sequences(X), met X de teksten\n",
    "\n",
    "Voor meer informatie over de preprocessing: https://keras.io/preprocessing/sequence/\n",
    "\n",
    "Niet alle teksten bevatten evenveel woorden. Dit resulteert in sequenties met verschillende lengte.\n",
    "Om sequenties met dezelfde lengte te bekomen is het nodig om padding toe te passen. In het voorbeeld pad_sequences(sequences, maxlen=100 padding='post') zal de maximale lengte van de sequenties 100 woorden bedragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omzetten van de labels naar numerieke waarden\n",
    "\n",
    "dataset['author'].replace({'MWS':0,'EAP':1,'HPL':2},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 97\n",
      "min 27\n"
     ]
    }
   ],
   "source": [
    "# Bepalen van de lengte van de langste tekst en van de kortste tekst\n",
    "\n",
    "print('max',len(dataset.text.max()))\n",
    "print('min',len(dataset.text.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc34b089710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAVYklEQVR4nO3df7DldX3f8edLVkSNyiK3W9xdu4xutZhEJXcAQ5oxEpeFti5JgcFJZSXb2fxBjDadNNg/ui2GGZ2kUrWVmZ2wujgGJKhhaxnJzqpNm8iPu0pRQMINiuwOsDfsij+o2GXe/eN8bjwu9/o9S+733L3c52PmzPl+39/P93vedy57X3x/nlQVkiT9NM9b7AYkScc+w0KS1MmwkCR1MiwkSZ0MC0lSpxWL3UAfTj755Fq3bt1ityFJS8revXv/tqom5lr2nAyLdevWMTU1tdhtSNKSkuSh+ZZ5GEqS1MmwkCR1MiwkSZ16DYsk/ybJPUm+nuT6JCckOTXJ7Ummk3wqyfFt7Ava/HRbvm5oO+9t9fuTnNtnz5KkZ+otLJKsBn4HmKyqnwWOAy4BPgBcXVWvBg4BW9oqW4BDrX51G0eS09p6rwM2Ah9NclxffUuSnqnvw1ArgBcmWQG8CHgEeAtwU1u+E7igTW9q87Tl5yRJq99QVU9V1TeBaeCMnvuWJA3pLSyqaj/wR8C3GYTEE8Be4DtVdbgN2wesbtOrgYfbuofb+JcP1+dY5+8k2ZpkKsnUzMzMwv9AkrSM9XkYaiWDvYJTgVcAL2ZwGKkXVbW9qiaranJiYs57SiRJz1Kfh6F+FfhmVc1U1f8DPgOcDZzYDksBrAH2t+n9wFqAtvxlwOPD9TnWkSSNQZ93cH8bOCvJi4D/C5wDTAFfBC4EbgA2Aze38bva/Jfb8i9UVSXZBfxJkg8y2ENZD9zRY99aQr595c8tdgvPea/8D19b7BZ0DOgtLKrq9iQ3AV8BDgNfBbYD/wO4IckftNq1bZVrgU8kmQYOMrgCiqq6J8mNwL1tO5dX1dN99S1JeqZenw1VVduAbUeUH2SOq5mq6ofARfNs5yrgqgVvUJI0Eu/gliR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdeotLJK8JsldQ6/vJnlPkpOS7E7yQHtf2cYnyYeTTCe5O8npQ9va3MY/kGRzXz1LkubWW1hU1f1V9YaqegPwC8CTwGeBK4A9VbUe2NPmAc4D1rfXVuAagCQnMfhq1jMZfB3rttmAkSSNx7gOQ50D/E1VPQRsAna2+k7ggja9CbiuBm4DTkxyCnAusLuqDlbVIWA3sHFMfUuSGF9YXAJc36ZXVdUjbfpRYFWbXg08PLTOvlabr/4TkmxNMpVkamZmZiF7l6Rlr/ewSHI88DbgT49cVlUF1EJ8TlVtr6rJqpqcmJhYiE1Kkppx7FmcB3ylqh5r84+1w0u09wOtvh9YO7Temlabry5JGpNxhMXb+fEhKIBdwOwVTZuBm4fql7aros4CnmiHq24FNiRZ2U5sb2g1SdKYrOhz40leDLwV+K2h8vuBG5NsAR4CLm71W4DzgWkGV05dBlBVB5O8D7izjbuyqg722bck6Sf1GhZV9QPg5UfUHmdwddSRYwu4fJ7t7AB29NGjJKmbd3BLkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI69RoWSU5MclOSbyS5L8mbkpyUZHeSB9r7yjY2ST6cZDrJ3UlOH9rO5jb+gSSb5/9ESVIf+t6z+BDw+ap6LfB64D7gCmBPVa0H9rR5gPOA9e21FbgGIMlJwDbgTOAMYNtswEiSxqO37+BO8jLgl4F3AlTVj4AfJdkEvLkN2wl8Cfh9YBNwXfsu7tvaXskpbezuqjrYtrsb2AhcvxB9/sLvXbcQm1GHvX946WK3IOnvoc89i1OBGeBjSb6a5I+TvBhYVVWPtDGPAqva9Grg4aH197XafHVJ0pj0GRYrgNOBa6rqjcAP+PEhJwDaXkQtxIcl2ZpkKsnUzMzMQmxSktT0GRb7gH1VdXubv4lBeDzWDi/R3g+05fuBtUPrr2m1+eo/oaq2V9VkVU1OTEws6A8iSctdb2FRVY8CDyd5TSudA9wL7AJmr2jaDNzcpncBl7aros4CnmiHq24FNiRZ2U5sb2g1SdKY9HaCu3kX8MkkxwMPApcxCKgbk2wBHgIubmNvAc4HpoEn21iq6mCS9wF3tnFXzp7sliSNR69hUVV3AZNzLDpnjrEFXD7PdnYAOxa2O0nSqLyDW5LUybCQJHUyLCRJnQwLSVKnvq+GkqR5nf2Rsxe7hee8v3zXXy7IdtyzkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR16jUsknwrydeS3JVkqtVOSrI7yQPtfWWrJ8mHk0wnuTvJ6UPb2dzGP5Bkc589S5KeaRx7Fr9SVW+oqtnv4r4C2FNV64E9bR7gPGB9e20FroFBuADbgDOBM4BtswEjSRqPxTgMtQnY2aZ3AhcM1a+rgduAE5OcApwL7K6qg1V1CNgNbBx305K0nPUdFgX8eZK9Sba22qqqeqRNPwqsatOrgYeH1t3XavPVf0KSrUmmkkzNzMws5M8gScte39+U90tVtT/JPwB2J/nG8MKqqiS1EB9UVduB7QCTk5MLsk1J0kCvexZVtb+9HwA+y+Ccw2Pt8BLt/UAbvh9YO7T6mlabry5JGpPewiLJi5O8ZHYa2AB8HdgFzF7RtBm4uU3vAi5tV0WdBTzRDlfdCmxIsrKd2N7QapKkMenzMNQq4LNJZj/nT6rq80nuBG5MsgV4CLi4jb8FOB+YBp4ELgOoqoNJ3gfc2cZdWVUHe+xbknSE3sKiqh4EXj9H/XHgnDnqBVw+z7Z2ADsWukdJ0mi8g1uS1MmwkCR1MiwkSZ0MC0lSJ8NCktRppLBIsmeUmiTpuemnXjqb5ATgRcDJ7Ya4tEUvZY7nM0mSnpu67rP4LeA9wCuAvfw4LL4L/Nce+5IkHUN+alhU1YeADyV5V1V9ZEw9SZKOMSPdwV1VH0nyi8C64XWq6rqe+pIkHUNGCosknwBeBdwFPN3KBRgWkrQMjPpsqEngtPb8JknSMjPqfRZfB/5hn41Iko5do+5ZnAzcm+QO4KnZYlW9rZeuJEnHlFHD4j/22YQk6dg26tVQ/7PvRiRJx65Rr4b6HoOrnwCOB54P/KCqXtpXY5KkY8dIJ7ir6iVV9dIWDi8E/iXw0VHWTXJckq8m+VybPzXJ7Ummk3wqyfGt/oI2P92Wrxvaxntb/f4k5x7lzyhJ+ns66qfO1sCfAaP+0X43cN/Q/AeAq6vq1cAhYEurbwEOtfrVbRxJTgMuAV4HbAQ+muS4o+1bkvTsjfrU2V8fel2Y5P3AD0dYbw3wz4A/bvMB3gLc1IbsBC5o05vaPG35OW38JuCGqnqqqr4JTANnjPTTSZIWxKhXQ/2LoenDwLcY/BHv8l+Afwe8pM2/HPhOVR1u8/v48dNrVwMPA1TV4SRPtPGrgduGtjm8zt9JshXYCvDKV75yhNYkSaMa9Wqoy452w0n+OXCgqvYmefPRrn+0qmo7sB1gcnLSO80laQGNehhqTZLPJjnQXp9uh5h+mrOBtyX5FnADg8NPHwJOTDIbUmuA/W16P7C2fd4K4GXA48P1OdaRJI3BqCe4PwbsYvC9Fq8A/nurzauq3ltVa6pqHYMT1F+oqt8Avghc2IZtBm5u07vaPG35F9qzqHYBl7SrpU4F1gN3jNi3JGkBjBoWE1X1sao63F4fByae5Wf+PvC7SaYZnJO4ttWvBV7e6r8LXAFQVfcANwL3Ap8HLq+qp5+xVUlSb0Y9wf14kn8FXN/m387gENFIqupLwJfa9IPMcTVTVf0QuGie9a8Crhr18yRJC2vUPYvfBC4GHgUeYXCY6J099SRJOsaMumdxJbC5qg4BJDkJ+CMGISJJeo4bdc/i52eDAqCqDgJv7KclSdKxZtSweF6SlbMzbc9i1L0SSdISN+of/P8MfDnJn7b5i/CEsyQtG6PewX1dkikGN9YB/HpV3dtfW5KkY8nIh5JaOBgQkrQMHfUjyiVJy49hIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSerUW1gkOSHJHUn+T5J7kvynVj81ye1JppN8Ksnxrf6CNj/dlq8b2tZ7W/3+JOf21bMkaW597lk8Bbylql4PvAHYmOQs4APA1VX1auAQsKWN3wIcavWr2ziSnAZcArwO2Ah8NMlxPfYtSTpCb2FRA99vs89vr2Lw5NqbWn0ncEGb3tTmacvPSZJWv6GqnqqqbwLTzPEd3pKk/vR6ziLJcUnuAg4Au4G/Ab5TVYfbkH3A6ja9GngYoC1/Anj5cH2OdYY/a2uSqSRTMzMzffw4krRs9RoWVfV0Vb0BWMNgb+C1PX7W9qqarKrJiYmJvj5GkpalsVwNVVXfAb4IvAk4Mcns92isAfa36f3AWoC2/GXA48P1OdaRJI1Bn1dDTSQ5sU2/EHgrcB+D0LiwDdsM3Nymd7V52vIvVFW1+iXtaqlTgfXAHX31LUl6ppG/Ke9ZOAXY2a5ceh5wY1V9Lsm9wA1J/gD4KnBtG38t8Ikk08BBBldAUVX3JLmRwbf0HQYur6qne+xbknSE3sKiqu4G3jhH/UHmuJqpqn4IXDTPtq4CrlroHiVJo/EObklSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqc+v4N7bZIvJrk3yT1J3t3qJyXZneSB9r6y1ZPkw0mmk9yd5PShbW1u4x9Isnm+z5Qk9aPPPYvDwL+tqtOAs4DLk5wGXAHsqar1wJ42D3AesL69tgLXwCBcgG3AmQy+jnXbbMBIksajt7Coqkeq6itt+nvAfcBqYBOwsw3bCVzQpjcB19XAbcCJSU4BzgV2V9XBqjoE7AY29tW3JOmZxnLOIsk64I3A7cCqqnqkLXoUWNWmVwMPD622r9Xmqx/5GVuTTCWZmpmZWdD+JWm56z0skvwM8GngPVX13eFlVVVALcTnVNX2qpqsqsmJiYmF2KQkqek1LJI8n0FQfLKqPtPKj7XDS7T3A62+H1g7tPqaVpuvLkkakz6vhgpwLXBfVX1waNEuYPaKps3AzUP1S9tVUWcBT7TDVbcCG5KsbCe2N7SaJGlMVvS47bOBdwBfS3JXq/174P3AjUm2AA8BF7dltwDnA9PAk8BlAFV1MMn7gDvbuCur6mCPfUuSjtBbWFTV/wYyz+Jz5hhfwOXzbGsHsGPhupMkHQ3v4JYkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHXq8zu4dyQ5kOTrQ7WTkuxO8kB7X9nqSfLhJNNJ7k5y+tA6m9v4B5JsnuuzJEn96nPP4uPAxiNqVwB7qmo9sKfNA5wHrG+vrcA1MAgXYBtwJnAGsG02YCRJ49NbWFTVXwAHjyhvAna26Z3ABUP162rgNuDEJKcA5wK7q+pgVR0CdvPMAJIk9Wzc5yxWVdUjbfpRYFWbXg08PDRuX6vNV5ckjdGineCuqgJqobaXZGuSqSRTMzMzC7VZSRLjD4vH2uEl2vuBVt8PrB0at6bV5qs/Q1Vtr6rJqpqcmJhY8MYlaTkbd1jsAmavaNoM3DxUv7RdFXUW8EQ7XHUrsCHJynZie0OrSZLGaEVfG05yPfBm4OQk+xhc1fR+4MYkW4CHgIvb8FuA84Fp4EngMoCqOpjkfcCdbdyVVXXkSXNJUs96C4uqevs8i86ZY2wBl8+znR3AjgVsTZJ0lLyDW5LUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1WjJhkWRjkvuTTCe5YrH7kaTlZEmERZLjgP8GnAecBrw9yWmL25UkLR9LIiyAM4Dpqnqwqn4E3ABsWuSeJGnZSFUtdg+dklwIbKyqf93m3wGcWVW/PTRmK7C1zb4GuH/sjY7PycDfLnYTetb8/S1dz/Xf3T+qqom5FqwYdyd9qartwPbF7mMckkxV1eRi96Fnx9/f0rWcf3dL5TDUfmDt0PyaVpMkjcFSCYs7gfVJTk1yPHAJsGuRe5KkZWNJHIaqqsNJfhu4FTgO2FFV9yxyW4tpWRxuew7z97d0Ldvf3ZI4wS1JWlxL5TCUJGkRGRaSpE6GxRLjY0+WriQ7khxI8vXF7kVHJ8naJF9Mcm+Se5K8e7F7GjfPWSwh7bEnfw28FdjH4Cqxt1fVvYvamEaS5JeB7wPXVdXPLnY/Gl2SU4BTquorSV4C7AUuWE7/9tyzWFp87MkSVlV/ARxc7D509Krqkar6Spv+HnAfsHpxuxovw2JpWQ08PDS/j2X2H6y02JKsA94I3L64nYyXYSFJI0ryM8CngfdU1XcXu59xMiyWFh97Ii2SJM9nEBSfrKrPLHY/42ZYLC0+9kRaBEkCXAvcV1UfXOx+FoNhsYRU1WFg9rEn9wE3LvPHniwpSa4Hvgy8Jsm+JFsWuyeN7GzgHcBbktzVXucvdlPj5KWzkqRO7llIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRZSz5JckOS0ofkvJZlczJ6ko2VYSP27ADitc9QIkiyJr0LWc49hIT0LSf4syd723QZbW+37Q8svTPLxJL8IvA34w3Yj16vakIuS3JHkr5P807bOCUk+luRrSb6a5Fda/Z1JdiX5ArBnvD+pNOD/pUjPzm9W1cEkLwTuTPLpuQZV1V8l2QV8rqpuAhg8OYIVVXVGuwt4G/CrwOWDVernkrwW+PMk/7ht6nTg56vKR5xrURgW0rPzO0l+rU2vBdYf5fqzD6LbC6xr078EfASgqr6R5CFgNix2GxRaTIaFdJSSvJnBnsCbqurJJF8CTgCGn51zQsdmnmrvTzPav8MfHGWb0oLynIV09F4GHGpB8VrgrFZ/LMk/SfI84NeGxn8PeMkI2/1fwG8AtMNPrwTuX7i2pWfPsJCO3ueBFUnuA94P3NbqVwCfA/4KeGRo/A3A77WT1q9ifh8Fnpfka8CngHdW1VM/Zbw0Nj51VpLUyT0LSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdfr/LkIRAY5b86cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gebalanceerdheid van de dataset\n",
    "\n",
    "g= dataset.groupby('author')\n",
    "g.author.count()\n",
    "sns.countplot(x='author',data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitsen in features en targets\n",
    "X = dataset['text'].values\n",
    "y = dataset['author'].values\n",
    "# one-hot encoding van de labels\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the net work was not permanently fastened to the hoop, but attached by a series of running loops or nooses.\n",
      "[51, 1, 6598, 344, 8, 20, 8548, 2356, 4, 1, 4932, 19, 1949, 24, 5, 1525, 2, 2269, 6599, 35, 12285]\n"
     ]
    }
   ],
   "source": [
    "# Sequenties genereren gebruik makende van de tokenizer\n",
    "tokenizer = Tokenizer(num_words= None)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Voorbeeld van een sequentie\n",
    "print(X[20])\n",
    "print(sequences[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   51,     1,  6598,   344,     8,    20,  8548,  2356,     4,\n",
       "           1,  4932,    19,  1949,    24,     5,  1525,     2,  2269,\n",
       "        6599,    35, 12285,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0], dtype=int32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Padding van de sequences (max_length = 100)\n",
    "X_encoded = pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "# Voorbeeld\n",
    "X_encoded[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitsen in training set en test set. Gebruik een test size = 3000 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=3000, random_state=0,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainen van een LSTM/GRU met een embedding layer. \n",
    "\n",
    "Input -> Embedding layer -> LSTM -> Dense output layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13263 samples, validate on 3316 samples\n",
      "Epoch 1/10\n",
      "13263/13263 [==============================] - 38s 3ms/sample - loss: 1.0893 - accuracy: 0.4029 - val_loss: 1.0883 - val_accuracy: 0.4035\n",
      "Epoch 2/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0878 - accuracy: 0.4035 - val_loss: 1.0890 - val_accuracy: 0.4035\n",
      "Epoch 3/10\n",
      "13263/13263 [==============================] - 36s 3ms/sample - loss: 1.0880 - accuracy: 0.4041 - val_loss: 1.0879 - val_accuracy: 0.4029\n",
      "Epoch 4/10\n",
      "13263/13263 [==============================] - 38s 3ms/sample - loss: 1.0890 - accuracy: 0.4053 - val_loss: 1.0877 - val_accuracy: 0.4032\n",
      "Epoch 5/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0852 - accuracy: 0.4057 - val_loss: 1.0877 - val_accuracy: 0.4032\n",
      "Epoch 6/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0831 - accuracy: 0.4059 - val_loss: 1.0860 - val_accuracy: 0.4041\n",
      "Epoch 7/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0819 - accuracy: 0.4062 - val_loss: 1.0865 - val_accuracy: 0.4038\n",
      "Epoch 8/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0822 - accuracy: 0.4062 - val_loss: 1.0852 - val_accuracy: 0.4044\n",
      "Epoch 9/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0809 - accuracy: 0.4064 - val_loss: 1.0877 - val_accuracy: 0.4044\n",
      "Epoch 10/10\n",
      "13263/13263 [==============================] - 37s 3ms/sample - loss: 1.0792 - accuracy: 0.4068 - val_loss: 1.0858 - val_accuracy: 0.4041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f87ac239e10>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = X_train.max()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, 150, input_length=100))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "#model.add(LSTM(200,return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10,verbose=1,batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "40.43333333333333\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.00      0.01       926\n",
      "           1       0.40      1.00      0.58      1211\n",
      "           2       0.00      0.00      0.00       863\n",
      "\n",
      "    accuracy                           0.40      3000\n",
      "   macro avg       0.38      0.33      0.19      3000\n",
      "weighted avg       0.39      0.40      0.23      3000\n",
      "\n",
      "\n",
      "\n",
      "[[   3  923    0]\n",
      " [   1 1210    0]\n",
      " [   0  863    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gevaertw/tensorflow/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Testen met de test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print('\\n')\n",
    "print(accuracy_score(y_true, y_pred) * 100)\n",
    "print('\\n')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('\\n')\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25944, 150)\n",
      "Word_index =  594\n",
      "lovely\n",
      "glassy\n",
      "five\n",
      "ferry\n",
      "stirred\n",
      "728\n"
     ]
    }
   ],
   "source": [
    "# Analyse van de word embeddings\n",
    "\n",
    "embeddings = model.layers[0].get_weights()\n",
    "embeddings = np.asarray(embeddings)\n",
    "embeddings = embeddings.squeeze(axis=0)\n",
    "print(embeddings.shape)\n",
    "\n",
    "word_list = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    word_list.append(word)\n",
    "    \n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "def find_word_in_dict(reverse_word_map, word): \n",
    "    items = reverse_word_map.values()\n",
    "    for i, j in enumerate(items):\n",
    "        if j == word:\n",
    "            return(i)\n",
    "\n",
    "#print(reverse_word_map)\n",
    "word = 'lovely'\n",
    "\n",
    "word_index = find_word_in_dict(reverse_word_map,word)\n",
    "print('Word_index = ', word_index)\n",
    "\n",
    "# word embeddingvector van het woord 'lovely'\n",
    "\n",
    "word_vector = np.array([embeddings[word_index]])\n",
    "\n",
    "\n",
    "# list met gesorteerde cosine similarities\n",
    "\n",
    "sim_list = []\n",
    "\n",
    "\n",
    "for v in embeddings:\n",
    "    similarity = (np.abs(cosine_similarity(word_vector, np.array([v])))).item()\n",
    "    sim_list.append(similarity)\n",
    "\n",
    "#print(simlist)\n",
    "      \n",
    "\n",
    "\n",
    "for i in range(0,5):\n",
    "    topword =  reverse_word_map.get(np.argmax(sim_list)+1)\n",
    "    print(topword)\n",
    "    sim_list.pop(np.argmax(sim_list))\n",
    "\n",
    "\n",
    "print(np.argmax(sim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vector loaded: 400000\n"
     ]
    }
   ],
   "source": [
    "#### GloVe 300d word embeddings\n",
    "embeddings_index = dict()\n",
    "f = open('./glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Number of word vector loaded: %s' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering:\n",
    "vocabulary_size = X_train.max()\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size+1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9947 samples, validate on 6632 samples\n",
      "Epoch 1/20\n",
      "9947/9947 [==============================] - 3s 309us/sample - loss: 1.0887 - accuracy: 0.4016 - val_loss: 1.0871 - val_accuracy: 0.4043\n",
      "Epoch 2/20\n",
      "9947/9947 [==============================] - 2s 221us/sample - loss: 1.0867 - accuracy: 0.4036 - val_loss: 1.0867 - val_accuracy: 0.4038\n",
      "Epoch 3/20\n",
      "9947/9947 [==============================] - 2s 219us/sample - loss: 1.0869 - accuracy: 0.4029 - val_loss: 1.0888 - val_accuracy: 0.4038\n",
      "Epoch 4/20\n",
      "9947/9947 [==============================] - 2s 214us/sample - loss: 1.0865 - accuracy: 0.4035 - val_loss: 1.0863 - val_accuracy: 0.4046\n",
      "Epoch 5/20\n",
      "9947/9947 [==============================] - 2s 206us/sample - loss: 1.0853 - accuracy: 0.4043 - val_loss: 1.0872 - val_accuracy: 0.4040\n",
      "Epoch 6/20\n",
      "9947/9947 [==============================] - 2s 218us/sample - loss: 1.0848 - accuracy: 0.4041 - val_loss: 1.0887 - val_accuracy: 0.4040\n",
      "Epoch 7/20\n",
      "9947/9947 [==============================] - 2s 225us/sample - loss: 1.0841 - accuracy: 0.4043 - val_loss: 1.0876 - val_accuracy: 0.4058\n",
      "Epoch 8/20\n",
      "9947/9947 [==============================] - 2s 225us/sample - loss: 1.0835 - accuracy: 0.4051 - val_loss: 1.0888 - val_accuracy: 0.4047\n",
      "Epoch 9/20\n",
      "9947/9947 [==============================] - 2s 225us/sample - loss: 1.0821 - accuracy: 0.4054 - val_loss: 1.0881 - val_accuracy: 0.4056\n",
      "Epoch 10/20\n",
      "9947/9947 [==============================] - 2s 230us/sample - loss: 1.0813 - accuracy: 0.4078 - val_loss: 1.0864 - val_accuracy: 0.4055\n",
      "Epoch 11/20\n",
      "9947/9947 [==============================] - 2s 222us/sample - loss: 1.0775 - accuracy: 0.4171 - val_loss: 1.0557 - val_accuracy: 0.4989\n",
      "Epoch 12/20\n",
      "9947/9947 [==============================] - 2s 224us/sample - loss: 1.0553 - accuracy: 0.4401 - val_loss: 1.0415 - val_accuracy: 0.4789\n",
      "Epoch 13/20\n",
      "9947/9947 [==============================] - 2s 228us/sample - loss: 1.0440 - accuracy: 0.4767 - val_loss: 1.0208 - val_accuracy: 0.5011\n",
      "Epoch 14/20\n",
      "9947/9947 [==============================] - 2s 222us/sample - loss: 1.0570 - accuracy: 0.4281 - val_loss: 1.0642 - val_accuracy: 0.3776\n",
      "Epoch 15/20\n",
      "9947/9947 [==============================] - 2s 226us/sample - loss: 1.0509 - accuracy: 0.4338 - val_loss: 1.0521 - val_accuracy: 0.4199\n",
      "Epoch 16/20\n",
      "9947/9947 [==============================] - 2s 211us/sample - loss: 1.0543 - accuracy: 0.4314 - val_loss: 1.0274 - val_accuracy: 0.4887\n",
      "Epoch 17/20\n",
      "9947/9947 [==============================] - 2s 208us/sample - loss: 1.0673 - accuracy: 0.4322 - val_loss: 1.0879 - val_accuracy: 0.4055\n",
      "Epoch 18/20\n",
      "9947/9947 [==============================] - 2s 209us/sample - loss: 1.0777 - accuracy: 0.4150 - val_loss: 1.0895 - val_accuracy: 0.4097\n",
      "Epoch 19/20\n",
      "9947/9947 [==============================] - 2s 212us/sample - loss: 1.0780 - accuracy: 0.4159 - val_loss: 1.0864 - val_accuracy: 0.4109\n",
      "Epoch 20/20\n",
      "9947/9947 [==============================] - 2s 210us/sample - loss: 1.0701 - accuracy: 0.4264 - val_loss: 1.0620 - val_accuracy: 0.4403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f878477dbd0>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, 300, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "vocabulary_size = X_train.max()\n",
    "model.fit(X_train, y_train, validation_split=0.4, epochs=20,verbose=1,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "43.9\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.16      0.26       926\n",
      "           1       0.42      0.97      0.58      1211\n",
      "           2       0.00      0.00      0.00       863\n",
      "\n",
      "    accuracy                           0.44      3000\n",
      "   macro avg       0.38      0.37      0.28      3000\n",
      "weighted avg       0.40      0.44      0.32      3000\n",
      "\n",
      "\n",
      "\n",
      "[[ 144  782    0]\n",
      " [  38 1173    0]\n",
      " [  14  849    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gevaertw/tensorflow/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Testen met de test set\n",
    "\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print('\\n')\n",
    "print(accuracy_score(y_true, y_pred) * 100)\n",
    "print('\\n')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('\\n')\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13263 samples, validate on 3316 samples\n",
      "Epoch 1/5\n",
      "13263/13263 [==============================] - 103s 8ms/sample - loss: 0.9922 - accuracy: 0.5175 - val_loss: 0.9144 - val_accuracy: 0.5850\n",
      "Epoch 2/5\n",
      "13263/13263 [==============================] - 100s 8ms/sample - loss: 0.8782 - accuracy: 0.6018 - val_loss: 0.8051 - val_accuracy: 0.6505\n",
      "Epoch 3/5\n",
      "13263/13263 [==============================] - 99s 7ms/sample - loss: 0.8147 - accuracy: 0.6404 - val_loss: 0.7585 - val_accuracy: 0.6906\n",
      "Epoch 4/5\n",
      "13263/13263 [==============================] - 99s 7ms/sample - loss: 0.7671 - accuracy: 0.6673 - val_loss: 0.6687 - val_accuracy: 0.7223\n",
      "Epoch 5/5\n",
      "13263/13263 [==============================] - 100s 8ms/sample - loss: 0.7203 - accuracy: 0.6985 - val_loss: 0.6585 - val_accuracy: 0.7289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f880cf31190>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bidirectional LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, 300, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3,return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5,verbose=1,batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "73.83333333333333\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74       926\n",
      "           1       0.75      0.75      0.75      1211\n",
      "           2       0.71      0.73      0.72       863\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.74      0.74      0.74      3000\n",
      "weighted avg       0.74      0.74      0.74      3000\n",
      "\n",
      "\n",
      "\n",
      "[[674 159  93]\n",
      " [144 907 160]\n",
      " [ 81 148 634]]\n"
     ]
    }
   ],
   "source": [
    "# Testen met de test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print('\\n')\n",
    "print(accuracy_score(y_true, y_pred) * 100)\n",
    "print('\\n')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('\\n')\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Airline Twitter sentiment\n",
    "\n",
    "De dataset 'Airlines_sentiment.csv' bevat tweets over verschillende airlines. Jouw taak bestaat erin om een classifier te trainen die zo goed mogelijk het sentiment van de tweets kan classificeren als positive, neutral en negative.\n",
    "Het sentiment bevindt zich in de kolom 'airline_sentiment' en de tweet zelf in de kolom 'text'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment:confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason:confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>681679794</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 19:46</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/22/15 12:01</td>\n",
       "      <td>5.695880e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>681679795</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 19:14</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>5.695870e+17</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>681679796</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 19:04</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>5.695870e+17</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>681679797</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 18:59</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>5.695870e+17</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>681679798</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 19:06</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/22/15 11:58</td>\n",
       "      <td>5.695870e+17</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "14635  681679794    False   finalized                   3     2/25/15 19:46   \n",
       "14636  681679795    False   finalized                   3     2/25/15 19:14   \n",
       "14637  681679796    False   finalized                   3     2/25/15 19:04   \n",
       "14638  681679797    False   finalized                   3     2/25/15 18:59   \n",
       "14639  681679798    False   finalized                   3     2/25/15 19:06   \n",
       "\n",
       "      airline_sentiment  airline_sentiment:confidence          negativereason  \\\n",
       "14635          positive                        0.3487                     NaN   \n",
       "14636          negative                        1.0000  Customer Service Issue   \n",
       "14637           neutral                        1.0000                     NaN   \n",
       "14638          negative                        1.0000  Customer Service Issue   \n",
       "14639           neutral                        0.6771                     NaN   \n",
       "\n",
       "       negativereason:confidence   airline airline_sentiment_gold  \\\n",
       "14635                     0.0000  American                    NaN   \n",
       "14636                     1.0000  American                    NaN   \n",
       "14637                        NaN  American                    NaN   \n",
       "14638                     0.6659  American                    NaN   \n",
       "14639                     0.0000  American                    NaN   \n",
       "\n",
       "                  name negativereason_gold  retweet_count  \\\n",
       "14635  KristenReenders                 NaN              0   \n",
       "14636         itsropes                 NaN              0   \n",
       "14637         sanyabun                 NaN              0   \n",
       "14638       SraJackson                 NaN              0   \n",
       "14639        daviddtwu                 NaN              0   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "14635  @AmericanAir thank you we got on a different f...         NaN   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...         NaN   \n",
       "14637  @AmericanAir Please bring American Airlines to...         NaN   \n",
       "14638  @AmericanAir you have my money, you change my ...         NaN   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...         NaN   \n",
       "\n",
       "       tweet_created      tweet_id tweet_location               user_timezone  \n",
       "14635  2/22/15 12:01  5.695880e+17            NaN                         NaN  \n",
       "14636  2/22/15 11:59  5.695870e+17          Texas                         NaN  \n",
       "14637  2/22/15 11:59  5.695870e+17  Nigeria,lagos                         NaN  \n",
       "14638  2/22/15 11:59  5.695870e+17     New Jersey  Eastern Time (US & Canada)  \n",
       "14639  2/22/15 11:58  5.695870e+17     dallas, TX                         NaN  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlezen van de dataset\n",
    "\n",
    "dataset = pd.read_csv('Airline_sentiment.csv',encoding = 'ISO-8859-1')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Onderzoek de gebalanceerdheid van de dataset. Stel grafisch de verdeling voor van het sentiment.\n",
    "- Visualiseer de verdeling van het sentiment per airline. Maak daarvoor per airline een Seaborn countplot van het sentiment.\n",
    "- Welke airline lijkt op basis van deze tweet het best te scoren en welke het slechtst?\n",
    "- Uit hoeveel woorden bestaat het langste twitter bericht en uit hoeveel woorden het kortste?\n",
    "- Onderzoek of er mogelijks foutieve of ontbrekende data aanwezig is. \n",
    "- Splits op in een training set en test set. Zorg ervoor dat er 3000 tweets in de test set steken.\n",
    "- Train een LSTM/GRU classifier die uit de tweet het sentiment zo nauwkeurig mogelijk kan voorspellen. Test op de test set.\n",
    "- Stel dat de airlines vooral geïnteresseerd zijn in het correct opsporen van negatieve tweets. Welke aanpassingen zou je kunnen doen om ervoor te zorgen dat het model minder negatieve tweets verkeerd classificeert? Test deze aanpassingen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitwerking Airline Twitter sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
